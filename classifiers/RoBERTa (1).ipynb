{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RoBERTa.ipynb (Thesis by Friso Stolk)\n"
      ],
      "metadata": {
        "id": "2ZgdCyPG-cGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PACKAGES"
      ],
      "metadata": {
        "id": "R4NaIP3WWeu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaToYnkbMLQY",
        "outputId": "4fa399bf-aa59-434e-f0fb-649c09eba831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 51.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 78.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.6.4-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.2.0)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.11.0+cu113)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.6)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 80.1 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.9.0-py3-none-any.whl (418 kB)\n",
            "\u001b[K     |████████████████████████████████| 418 kB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 80.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.46.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 79.4 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 85.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, pyDeprecate, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.4 torchmetrics-0.9.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch_lightning\n",
        "from concurrent.futures import thread\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "import pytorch_lightning as pl\n",
        "import transformers\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET CLASS"
      ],
      "metadata": {
        "id": "ClbBz6P2Wi4U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX9YH7yCSWSj",
        "outputId": "5fb1d8f2-2a7f-4718-c122-7c101cb6bd8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "class EvidenceTypeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, thread_id, sentence, target, tokenizer, max_len):\n",
        "        self.thread_id = thread_id\n",
        "        self.sentence = sentence\n",
        "        self.targets = target\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentence)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sentence = str(self.sentence[item])\n",
        "        thread_id = str(self.thread_id[item])\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            max_length=self.max_len,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'thread_id': thread_id,\n",
        "            'sentence_text': sentence,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'targets': torch.tensor(self.targets[item], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MACHINE LEARNING CLASS"
      ],
      "metadata": {
        "id": "AWdbYGEZWlQZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AaSmAOVnShYs"
      },
      "outputs": [],
      "source": [
        "class EvidenceTypeClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(EvidenceTypeClassifier, self).__init__()\n",
        "        # defines model\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base', return_dict=False)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATALOADER"
      ],
      "metadata": {
        "id": "K6WIFEVQWqvs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q5-L3DZQUK0B"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "    # creates dataloader to return the right foramt\n",
        "    return DataLoader(\n",
        "        EvidenceTypeDataset(\n",
        "            thread_id=df.thread_id.to_numpy(),\n",
        "            sentence=df.sentence.to_numpy(),\n",
        "            target=df.label.to_numpy(),\n",
        "            tokenizer=tokenizer,\n",
        "            max_len=max_len\n",
        "        ),\n",
        "        batch_size=batch_size,\n",
        "        num_workers=4\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINER"
      ],
      "metadata": {
        "id": "WiEgzI4BWvbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pYL2bzbGURT5"
      },
      "outputs": [],
      "source": [
        "def train_model(model, data_loader, loss_function, optimizer, scheduler, num_examples):\n",
        "    # Trains model\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d['input_ids'].to(device)\n",
        "        attention_mask = d['attention_mask'].to(device)\n",
        "        targets = d['targets'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / num_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION OF MODEL"
      ],
      "metadata": {
        "id": "vn1eqvhNW0Fa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w0__ihM8CkwF"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, loss_function, num_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d['input_ids']\n",
        "            attention_mask = d['attention_mask']\n",
        "            targets = d['targets']\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = loss_function(outputs, targets)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / num_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIND BEST MODEL"
      ],
      "metadata": {
        "id": "wcLQjrLxW2FD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sPisA12KUekH"
      },
      "outputs": [],
      "source": [
        "def find_best_model(model, loss_function, df_train, df_val, tokenizer, max_len, batch_size, num_epochs, print_graph=True, save_file_name='best_model_state.bin'):\n",
        "\n",
        "    train_data_loader = create_data_loader(df_train, tokenizer, max_len, batch_size)\n",
        "    val_data_loader = create_data_loader(df_val, tokenizer, max_len, batch_size)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False, no_deprecation_warning=True)\n",
        "\n",
        "    total_steps = len(train_data_loader) * num_epochs\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 40)\n",
        "\n",
        "        training_accuracy, training_loss = train_model(\n",
        "            model,\n",
        "            train_data_loader,\n",
        "            loss_function,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            len(df_train)\n",
        "        )\n",
        "\n",
        "        print('Training:   loss {:.3f} - accuracy {:.3f}'.format(training_loss, training_accuracy))\n",
        "\n",
        "        validation_accuracy, validation_loss = evaluate_model(\n",
        "            model,\n",
        "            val_data_loader,\n",
        "            loss_function,\n",
        "            len(df_val)\n",
        "        )\n",
        "\n",
        "        print('Validation: loss {:.3f} - accuracy {:.3f}\\n'.format(validation_loss, validation_accuracy))\n",
        "\n",
        "        history['training_accuracy'].append(training_accuracy)\n",
        "        history['training_loss'].append(training_loss)\n",
        "\n",
        "        history['validation_accuracy'].append(validation_accuracy)\n",
        "        history['validation_loss'].append(validation_loss)\n",
        "\n",
        "        if validation_accuracy > best_accuracy:\n",
        "            torch.save(model.state_dict(), save_file_name)\n",
        "            best_accuracy = validation_accuracy\n",
        "            with open('/content/gdrive/My Drive/best_model.bin', 'w') as f:\n",
        "              torch.save(model.state_dict(), \"/content/gdrive/My Drive/best_model.bin\")\n",
        "\n",
        "    if print_graph:\n",
        "        plt.plot(history['training_accuracy'], label='training accuracy')\n",
        "        plt.plot(history['validation_accuracy'], label='validation accuracy')\n",
        "        plt.title('Training history')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.ylim([0, 1])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREDICT LABELS"
      ],
      "metadata": {
        "id": "OzBhemMzW38R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ciK1Pv09Ut2C"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader, label_encoder, save_predictions=True, save_file_name='predictions.csv'):\n",
        "    model = model.eval()\n",
        "    thread_id_list = []\n",
        "    comment_id_list = []\n",
        "    sentence_texts = []\n",
        "    predictions = []\n",
        "    real_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            thread_ids = d['thread_id']\n",
        "            # comment_ids = d['comment_id']\n",
        "            texts = d[\"sentence_text\"]\n",
        "            input_ids = d[\"input_ids\"]\n",
        "            attention_mask = d[\"attention_mask\"]\n",
        "            targets = d[\"targets\"]\n",
        "            outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "            )\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            thread_id_list.extend(thread_ids)\n",
        "            #comment_id_list.extend(comment_ids)\n",
        "            sentence_texts.extend(texts)\n",
        "            predictions.extend(preds)\n",
        "            real_values.extend(targets)\n",
        "\n",
        "    if save_predictions:\n",
        "        df_pred = pd.DataFrame()\n",
        "        df_pred['thread_id'] = thread_id_list\n",
        "        # df_pred['comment_id'] = comment_id_list\n",
        "        df_pred['sentence'] = sentence_texts\n",
        "        df_pred['pred_label'] = label_encoder.inverse_transform(predictions)\n",
        "        df_pred['real_label'] = label_encoder.inverse_transform(real_values)\n",
        "\n",
        "        df_pred.to_csv(save_file_name, index=False)\n",
        "\n",
        "    return thread_id_list, comment_id_list, sentence_texts, predictions, real_values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPARE DATA"
      ],
      "metadata": {
        "id": "BOwpgJTWW8J2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eEk0MeSqOO2G"
      },
      "outputs": [],
      "source": [
        "# reads dataset and split it into train dev and test set\n",
        "def prepare_data(dataset):\n",
        "  # prepares data and shuffle it in 30 threads train 10 threads dev and 10 threads test\n",
        "  dataset.head()\n",
        "  thread_ids = dataset[\"thread_id\"].unique()\n",
        "  random.Random(10).shuffle(thread_ids)\n",
        "\n",
        "  train_ids = thread_ids[:30]\n",
        "  dev_ids = thread_ids[30:40]\n",
        "  test_ids = thread_ids[40:50]\n",
        "\n",
        "  train = dataset.loc[dataset[\"thread_id\"].isin(train_ids)]\n",
        "  test = dataset.loc[dataset[\"thread_id\"].isin(test_ids)]\n",
        "  dev = dataset.loc[dataset[\"thread_id\"].isin(dev_ids)]\n",
        "  return(train,test,dev)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN FUNCTION"
      ],
      "metadata": {
        "id": "gHHgWyAXXlQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpX3iFXqkIxB",
        "outputId": "24eb871b-7a4e-4721-851e-3c8219e66a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "        Anecdote       0.00      0.00      0.00       144\n",
            "      Assumption       0.58      1.00      0.73       763\n",
            "      Definition       0.00      0.00      0.00        30\n",
            "            None       0.00      0.00      0.00       323\n",
            "           Other       0.00      0.00      0.00        20\n",
            "Statistics/Study       0.00      0.00      0.00        18\n",
            "       Testimony       0.00      0.00      0.00        20\n",
            "\n",
            "        accuracy                           0.58      1318\n",
            "       macro avg       0.08      0.14      0.10      1318\n",
            "    weighted avg       0.34      0.58      0.42      1318\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    # reads dataset annotations\n",
        "    df = pd.read_csv('final_dataset.csv')\n",
        "\n",
        "\n",
        "\n",
        "    # Transform labels to integers\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['label'] = label_encoder.fit_transform(df['evidence_no_continue'])\n",
        "\n",
        "    # define tokenizer\n",
        "    from transformers import AutoTokenizer\n",
        "    model_name = 'roberta-base'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "    #loss_function = SelfAdjDiceLoss()\n",
        "\n",
        "    # gest split between train validation and test data\n",
        "    df_train, df_val, df_test = prepare_data(df)\n",
        "    #compute the class weights\n",
        "    # class_weights = (1 - (df['label'].value_counts().sort_index() / len(df))).values\n",
        "    # class_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\n",
        "    # loss_function = nn.CrossEntropyLoss(weight=class_weights).to(\"cuda\")\n",
        "    # define loss function\n",
        "    loss_function = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # hyper parameters\n",
        "    max_len = 100\n",
        "    batch_size = 64\n",
        "    num_epochs = 10\n",
        "\n",
        "    # number of different labels\n",
        "    num_classes = len(df['label'].unique())\n",
        "    # creates right input\n",
        "    test_data_loader = create_data_loader(df_test, tokenizer, max_len, batch_size)\n",
        "    # Define model\n",
        "    model = EvidenceTypeClassifier(num_classes)\n",
        "    # finds the best model\n",
        "    #find_best_model(model, loss_function, df_train, df_val, tokenizer, max_len, batch_size, num_epochs)\n",
        "\n",
        "    # define best model\n",
        "    model = EvidenceTypeClassifier(num_classes)\n",
        "    model.load_state_dict(torch.load('best_model_state.bin'))\n",
        "    model = model\n",
        "    \n",
        "    # gets predictions\n",
        "    thread_ids, comment_ids, y_sentence_texts, y_pred, y_test = get_predictions(\n",
        "    model,\n",
        "    test_data_loader,\n",
        "    label_encoder,\n",
        "    save_file_name='pred1.csv'\n",
        "    )\n",
        "\n",
        "    # gets results\n",
        "    y_test = label_encoder.inverse_transform(y_test)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "    # print classification report\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    X = list(df_test[\"sentence\"])\n",
        "    Y = list(df_test[\"evidence_no_continue\"])\n",
        "    Y = label_encoder.inverse_transform(df_test[\"label\"])\n",
        "    from sklearn.dummy import DummyClassifier\n",
        "    dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
        "    dummy_clf.fit(X, Y)\n",
        "    Ypred = dummy_clf.predict(X)\n",
        "    print(classification_report(Y, Ypred))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "RoBERTa.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}